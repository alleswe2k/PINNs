{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA\n"
     ]
    }
   ],
   "source": [
    "# CUDA support\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CPU\")\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.layers.append(nn.Linear(layers[i], layers[i+1]))\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.activation(layer(x))\n",
    "        return self.layers[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 7.02e10\n",
    "I = 0.1**4 / 12\n",
    "L = 2.0\n",
    "Q = -1e4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN():\n",
    "    def __init__(self, X, layers):\n",
    "        \n",
    "        self.x = torch.tensor(X, requires_grad=True).float().to(device)\n",
    "\n",
    "        # DNN\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer_lfbgs = torch.optim.LBFGS(\n",
    "            self.dnn.parameters(),\n",
    "            lr=0.1,\n",
    "            max_iter=50000,\n",
    "            max_eval=50000,\n",
    "            history_size=50,\n",
    "            tolerance_grad=1e-7,\n",
    "            tolerance_change=1.0 * np.finfo(float).eps,\n",
    "            line_search_fn=\"strong_wolfe\"\n",
    "        )\n",
    "\n",
    "        self.optimizer_adam = torch.optim.Adam(self.dnn.parameters(), lr=0.01)\n",
    "        self.iter = 0\n",
    "\n",
    "    def model_value(self, x):\n",
    "        u = self.dnn(x)\n",
    "        return u\n",
    "    \n",
    "    def loss_func(self, x):\n",
    "        u = self.model_value(x)\n",
    "        u_x = torch.autograd.grad(u, x, torch.ones_like(u), create_graph=True)[0]\n",
    "        u_2x = torch.autograd.grad(u_x, x, torch.ones_like(u_x), create_graph=True)[0]\n",
    "        u_3x = torch.autograd.grad(u_2x, x, torch.ones_like(u_2x), create_graph=True)[0]\n",
    "        u_4x = torch.autograd.grad(u_3x, x, torch.ones_like(u_3x), create_graph=True)[0]\n",
    "\n",
    "        # BC\n",
    "        bc_loss = torch.mean(u[0]**2)\n",
    "        bc_loss += torch.mean(u_x[0]**2)\n",
    "        bc_loss += torch.mean(u_2x[-1]**2)\n",
    "        bc_loss += torch.mean((E*I*u_3x[-1])**2)        \n",
    "\n",
    "        # PDE\n",
    "        residual = torch.mean((E * I * u_4x - Q)**2)\n",
    "\n",
    "        return residual + bc_loss\n",
    "    \n",
    "    def lbfgs_func(self):\n",
    "        loss = self.loss_func(self.x)\n",
    "\n",
    "        self.optimizer_lfbgs.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        self.iter += 1\n",
    "        if self.iter % 100 == 0:\n",
    "            print(f\"Iter: {self.iter}, Loss: {loss.item():.9f}\")\n",
    "        return loss\n",
    "    \n",
    "    def train(self, epochs=1000):\n",
    "        self.dnn.train()\n",
    "        for epoch in range(epochs):\n",
    "            loss = self.loss_func(self.x)\n",
    "\n",
    "            self.optimizer_adam.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer_adam.step()\n",
    "\n",
    "            if epoch % 200 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss.item():.9f}\")\n",
    "\n",
    "        self.optimizer_lfbgs.step(self.lbfgs_func)\n",
    "\n",
    "    def predict(self, X):\n",
    "        x = torch.tensor(X, requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval()\n",
    "        u = self.model_value(x)\n",
    "        u = u.detach().cpu().numpy()\n",
    "        return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 14386556928.000000000\n",
      "Epoch 200, Loss: 112410.835937500\n",
      "Epoch 400, Loss: 104753.906250000\n",
      "Epoch 600, Loss: 96390.218750000\n",
      "Epoch 800, Loss: 87567.468750000\n",
      "Epoch 1000, Loss: 78553.687500000\n",
      "Epoch 1200, Loss: 69465.617187500\n",
      "Epoch 1400, Loss: 60400.917968750\n",
      "Epoch 1600, Loss: 51477.941406250\n",
      "Epoch 1800, Loss: 42836.664062500\n",
      "Iter: 100, Loss: 2262.838867188\n",
      "Iter: 200, Loss: 310.732910156\n"
     ]
    }
   ],
   "source": [
    "layers = [1, 20, 20, 20, 1]\n",
    "\n",
    "x_t = np.random.uniform(0, L, 100)\n",
    "x_t = np.append(x_t, [0, L])\n",
    "x_t = np.sort(x_t).reshape(-1, 1)\n",
    "\n",
    "model = PINN(x_t, layers)\n",
    "model.train(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exact solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = lambda x: Q / (24*EI) * (x**4 - 4*L*x**3 + 6*L**2*x**2) # Fixed, free, constant q\n",
    "q2 = lambda x: Q / (120*EI) * (x**5/L - 10*L*x**3 + 20*L**2*x**2) # Fixed, free, increasing q\n",
    "q3 = lambda x: Q / (120*EI) * (-x**5/L + 5*x**4 - 10*L*x**3 + 10*L**2*x**2) # Fixed, free, decreasing q\n",
    "q4 = lambda x: Q / (24*EI) * (x**4/L - 2*x**3 + x/L**2) # Pinned, pinned, constant q\n",
    "q5 = lambda x: Q / (180*EI) * (3*x**5/L**2 - 10*x**3 + 7*L**2*x) # Pinned, pinned, increasing q\n",
    "q6 = lambda x: Q / (180*EI) * (-3*x**5/L**2 + 15*x**4/L - 20*x**3 + 8*L**2*x) # Pinned, pinned, decreasing q\n",
    "q7 = lambda x: Q / (48*EI) * (2*x**4 - 5*L*x**3 + 3*L**2*x**2) # Fixed, pinned, constant q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m,L,\u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[0;32m      4\u001b[0m u_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "x = np.linspace(0,L,100).reshape(-1, 1)\n",
    "\n",
    "# Predict\n",
    "u_pred = model.predict(x)\n",
    "\n",
    "# Exact\n",
    "u_exact = exact_solution(x)\n",
    "\n",
    "u_error = np.linalg.norm(u_pred - u_exact) / np.linalg.norm(u_exact,2)\n",
    "abs_error = np.abs(u_pred - u_exact)\n",
    "print(f\"L2 Error: {u_error}\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 5), sharex=True)\n",
    "\n",
    "ax1.plot(x, u_pred, label=\"PINN Solution\")\n",
    "ax1.plot(x, u_exact, label=\"Exact Solution\", linestyle=\"dashed\")\n",
    "ax1.set(ylabel='w(x)')\n",
    "ax1.legend()\n",
    "ax1.grid()\n",
    "\n",
    "ax2.plot(x, abs_error, 'tab:red')\n",
    "ax2.set(xlabel='x', ylabel='Absolute error')\n",
    "ax2.grid()\n",
    "\n",
    "formatter = mticker.ScalarFormatter(useMathText=True)\n",
    "formatter.set_scientific(True)\n",
    "formatter.set_powerlimits((-2, 2))  # Defines when to switch to scientific notation\n",
    "\n",
    "ax2.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
