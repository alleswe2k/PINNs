{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.current_device())  # Should return an integer (GPU index)\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))  # GPU name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA\n"
     ]
    }
   ],
   "source": [
    "# CUDA support\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CPU\")\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.layers.append(nn.Linear(layers[i], layers[i+1]))\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.activation(layer(x))\n",
    "        return self.layers[-1](x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbfgs_lr = 0.1\n",
    "adam_lr = 0.01\n",
    "\n",
    "w_pde = 1\n",
    "w_bc = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN():\n",
    "    def __init__(self, X, layers, lb, rb, q_c):\n",
    "        \n",
    "        self.x = torch.tensor(X, requires_grad=True).float().to(device)\n",
    "        self.q_c = torch.tensor(q_c).float().to(device)\n",
    "\n",
    "        self.lb = lb\n",
    "        self.rb = rb\n",
    "\n",
    "        # DNN\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer_lbfgs = torch.optim.LBFGS(\n",
    "            self.dnn.parameters(),\n",
    "            lr=lbfgs_lr,\n",
    "            max_iter=50000,\n",
    "            max_eval=50000,\n",
    "            history_size=50,\n",
    "            tolerance_grad=1e-7,\n",
    "            tolerance_change=1.0 * np.finfo(float).eps,\n",
    "            line_search_fn=\"strong_wolfe\"\n",
    "        )\n",
    "\n",
    "        self.optimizer_adam = torch.optim.Adam(self.dnn.parameters(), lr=adam_lr)\n",
    "        self.iter = 0\n",
    "        self.loss_count = 0\n",
    "\n",
    "    def model_value(self, x):\n",
    "        u = self.dnn(x)\n",
    "        return u\n",
    "    \n",
    "    def boundary_condition(self, cond, u, u_x, u_2x, u_3x):\n",
    "        bc_loss = 0\n",
    "\n",
    "        match cond:\n",
    "            case 'pinned':\n",
    "                bc_loss += u**2 + u_2x**2\n",
    "            case 'fixed':\n",
    "                bc_loss += u**2 + u_x**2\n",
    "            case 'free':\n",
    "                bc_loss += u_2x**2 + u_3x**2\n",
    "            case 'roller':\n",
    "                bc_loss += u_x**2 + u_3x**2\n",
    "\n",
    "        return bc_loss\n",
    "    \n",
    "    def loss_func(self, x):\n",
    "        self.loss_count += 1\n",
    "        u = self.model_value(x)\n",
    "        u_x = torch.autograd.grad(u, x, torch.ones_like(u), create_graph=True)[0]\n",
    "        u_2x = torch.autograd.grad(u_x, x, torch.ones_like(u_x), create_graph=True)[0]\n",
    "        u_3x = torch.autograd.grad(u_2x, x, torch.ones_like(u_2x), create_graph=True)[0]\n",
    "        u_4x = torch.autograd.grad(u_3x, x, torch.ones_like(u_3x), create_graph=True)[0]\n",
    "\n",
    "        # BC\n",
    "        bc_loss = self.boundary_condition(self.lb, u[0], u_x[0], u_2x[0], u_3x[0])\n",
    "        bc_loss += self.boundary_condition(self.rb, u[-1], u_x[-1], u_2x[-1], u_3x[-1])\n",
    "\n",
    "        # PDE\n",
    "        pde_loss = torch.mean((u_4x - self.q_c)**2)\n",
    "\n",
    "        return pde_loss, bc_loss\n",
    "\n",
    "\n",
    "    def lbfgs_func(self):\n",
    "        pde_loss, bc_loss = self.loss_func(self.x)\n",
    "        loss = w_pde*pde_loss + w_bc*bc_loss\n",
    "\n",
    "        self.optimizer_lbfgs.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        \n",
    "        if self.iter % 500 == 0:\n",
    "            print(f\"Iter: {self.iter}, PDE loss: {'{:e}'.format(pde_loss.item())}, BC loss: {'{:e}'.format(bc_loss.item())}\")\n",
    "        self.iter += 1\n",
    "        return loss\n",
    "    \n",
    "    def train(self, epochs=1000):\n",
    "        self.dnn.train()\n",
    "        for epoch in range(epochs):\n",
    "            pde_loss, bc_loss = self.loss_func(self.x)\n",
    "            loss = w_pde*pde_loss + w_bc*bc_loss\n",
    "\n",
    "            self.optimizer_adam.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer_adam.step()\n",
    "\n",
    "            if epoch % 500 == 0:\n",
    "                print(f\"Epoch: {epoch}, PDE loss: {'{:e}'.format(pde_loss.item())}, BC loss: {'{:e}'.format(bc_loss.item())}\")\n",
    "\n",
    "        self.optimizer_lbfgs.step(self.lbfgs_func)\n",
    "\n",
    "    def predict(self, X, q_c, L, EI):\n",
    "        x = torch.tensor(X, requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval()\n",
    "        u_c = self.model_value(x)\n",
    "        u_c = u_c.detach().cpu().numpy()\n",
    "        u = (q_c * L**4 / (EI)) * u_c\n",
    "\n",
    "        pde_loss, bc_loss = self.loss_func(self.x)\n",
    "        final_loss = w_pde*pde_loss + w_bc*bc_loss\n",
    "        final_loss = final_loss.detach().cpu().numpy()\n",
    "        return u, final_loss[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "964166.6666666669\n"
     ]
    }
   ],
   "source": [
    "EI = 11.57e10*0.1**4/12\n",
    "Q = -1.0e6\n",
    "L = 3.0\n",
    "\n",
    "print(EI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exact solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = lambda x: Q / (24*EI) * (x**4 - 4*L*x**3 + 6*L**2*x**2) # Fixed, free, constant q\n",
    "q2 = lambda x: Q / (120*EI) * (x**5/L - 10*L*x**3 + 20*L**2*x**2) # Fixed, free, increasing q\n",
    "q3 = lambda x: Q / (120*EI) * (-x**5/L + 5*x**4 - 10*L*x**3 + 10*L**2*x**2) # Fixed, free, decreasing q\n",
    "q4 = lambda x: Q / (24*EI) * (x**4/L - 2*x**3 + x/L**2) # Pinned, pinned, constant q\n",
    "q5 = lambda x: Q / (180*EI) * (3*x**5/L**2 - 10*x**3 + 7*L**2*x) # Pinned, pinned, increasing q\n",
    "q6 = lambda x: Q / (180*EI) * (-3*x**5/L**2 + 15*x**4/L - 20*x**3 + 8*L**2*x) # Pinned, pinned, decreasing q\n",
    "q7 = lambda x: Q / (48*EI) * (2*x**4 - 5*L*x**3 + 3*L**2*x**2) # Fixed, pinned, constant q\n",
    "exact_dict = {\n",
    "    \"fixed_free_constant\": q1,\n",
    "    \"fixed_free_increasing\": q2,\n",
    "    \"fixed_free_decreasing\": q3,\n",
    "    \"pinned_pinned_constant\": q4,\n",
    "    \"pinned_pinned_increasing\": q5,\n",
    "    \"pinned_pinned_decreasing\": q6,\n",
    "    \"fixed_pinned_constant\": q7,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "qx1 = lambda x: np.full_like(x, Q/L) # Constant\n",
    "qx2 = lambda x: Q*2/L**2 * x # Increasing\n",
    "qx3 = lambda x: Q*2/L**2 * (L - x) # Decrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 1000\n",
    "epochs = 5000\n",
    "\n",
    "x_t = np.random.uniform(0, L, n_points-2)\n",
    "# x_t = np.linspace(0, L, n_points-2)\n",
    "x_t = np.append(x_t, [0, L])\n",
    "x_t = np.sort(x_t).reshape(-1, 1)\n",
    "\n",
    "q = qx2(x_t)\n",
    "\n",
    "x_hat = x_t / L\n",
    "q_c = np.max(np.abs(q))\n",
    "\n",
    "q_hat = q / q_c\n",
    "\n",
    "\n",
    "nodes = 20\n",
    "layers = [1, nodes, nodes, nodes, 1]\n",
    "lb = 'pinned'\n",
    "rb = 'pinned'\n",
    "\n",
    "model = PINN(x_hat, layers, lb, rb, q_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=20, bias=True)\n",
      "    (1-2): 2 x Linear(in_features=20, out_features=20, bias=True)\n",
      "    (3): Linear(in_features=20, out_features=1, bias=True)\n",
      "  )\n",
      "  (activation): Tanh()\n",
      ")\n",
      "Total trainable parameters: 901\n"
     ]
    }
   ],
   "source": [
    "print(model.dnn)\n",
    "num_params = sum(p.numel() for p in model.dnn.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(10, 10, device=device)\n",
    "b = torch.randn(10, 10, device=device)\n",
    "\n",
    "# Warm-up: run a dummy operation to initialize the GPU and CUDA context\n",
    "c = torch.mm(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, PDE loss: 3.972704e-06, BC loss: 3.201536e-07\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed eval>:1\u001b[0m\n",
      "Cell \u001b[1;32mIn[6], line 86\u001b[0m, in \u001b[0;36mPINN.train\u001b[1;34m(self, epochs)\u001b[0m\n\u001b[0;32m     83\u001b[0m loss \u001b[38;5;241m=\u001b[39m w_pde\u001b[38;5;241m*\u001b[39mpde_loss \u001b[38;5;241m+\u001b[39m w_bc\u001b[38;5;241m*\u001b[39mbc_loss\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_adam\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 86\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_adam\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\alexa\\miniconda3\\envs\\pinn\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\miniconda3\\envs\\pinn\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\miniconda3\\envs\\pinn\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, L, n_points).reshape(-1, 1)\n",
    "u_pred, final_loss = model.predict(x/L, q_c, L, EI)\n",
    "u_pred = u_pred.flatten()\n",
    "u_exact = q5(x.flatten())\n",
    "\n",
    "u_error = np.linalg.norm(u_pred - u_exact) / np.linalg.norm(u_exact,2)\n",
    "abs_error = np.abs(u_pred - u_exact)\n",
    "max_error = np.max(abs_error)\n",
    "\n",
    "\n",
    "print(f\"L2 Error: {u_error:.3e}\")\n",
    "print(f\"Final loss: {final_loss:.3e}\")\n",
    "print(f\"Max absolute error: {max_error:.3e}\")\n",
    "print(model.loss_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 5), sharex=True)\n",
    "\n",
    "ax1.plot(x, u_pred, label=\"PINN Solution\")\n",
    "ax1.plot(x, u_exact, label=\"Exact Solution\", linestyle=\"dashed\")\n",
    "ax1.set(ylabel='w(x)')\n",
    "ax1.legend()\n",
    "ax1.grid()\n",
    "\n",
    "ax2.plot(x, abs_error, 'tab:red')\n",
    "ax2.set(xlabel='x', ylabel='Absolute error')\n",
    "ax2.grid()\n",
    "\n",
    "formatter = mticker.ScalarFormatter(useMathText=True)\n",
    "formatter.set_scientific(True)\n",
    "formatter.set_powerlimits((-2, 2))  # Defines when to switch to scientific notation\n",
    "\n",
    "ax2.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_file = \"pinn_experiments.csv\"\n",
    "\n",
    "# # Define the parameters to log\n",
    "# experiment_data = {\n",
    "#     \"learning_rate\": [adam_lr, lbfgs_lr],\n",
    "#     \"num_epochs\": epochs,\n",
    "#     \"num_points\": n_points,\n",
    "#     \"hidden_layers\": layers[1:-1],\n",
    "#     \"weight_pde\": w_pde,\n",
    "#     \"weight_bc\": w_bc,\n",
    "#     \"final_loss\": \"{:.3e}\".format(final_loss),\n",
    "#     \"max_abs_error\": \"{:.3e}\".format(max_error),\n",
    "#     \"L2_error\": \"{:.3e}\".format(u_error),\n",
    "#     \"train_time\": \"{:.3f}\".format(trainning_time)\n",
    "# }\n",
    "\n",
    "# # Check if file exists to write the header only once\n",
    "# file_exists = os.path.isfile(log_file)\n",
    "\n",
    "# with open(log_file, mode=\"a\", newline=\"\") as file:\n",
    "#     writer = csv.DictWriter(file, fieldnames=experiment_data.keys())\n",
    "    \n",
    "#     # Write the header if it's a new file\n",
    "#     if not file_exists:\n",
    "#         writer.writeheader()\n",
    "    \n",
    "#     # Write experiment data\n",
    "#     writer.writerow(experiment_data)\n",
    "\n",
    "# print(\"Experiment logged successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
