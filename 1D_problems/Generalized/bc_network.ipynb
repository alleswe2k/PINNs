{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2060\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))  # GPU name\n",
    "\n",
    "# CUDA support\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamData(Dataset):\n",
    "    def __init__(self, num_points, beam_configs=None):\n",
    "        \n",
    "\n",
    "        self.data = []\n",
    "\n",
    "        for config in beam_configs:\n",
    "            x = config['x']\n",
    "            lbc = torch.tensor(config['lbc'], dtype=torch.float).repeat(num_points, 1)\n",
    "            rbc = torch.tensor(config['rbc'], dtype=torch.float).repeat(num_points, 1)\n",
    "\n",
    "            self.data.append(torch.cat((x, lbc, rbc), dim=1))\n",
    "\n",
    "        self.data = torch.cat(self.data, dim=0).requires_grad_().to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 100\n",
    "x = torch.linspace(0, 1, p).view(-1, 1)\n",
    "\n",
    "fixed = [1, 0, 0]\n",
    "pinned = [0, 1, 0]\n",
    "free = [0, 0, 1]\n",
    "\n",
    "layers = [7, 16, 2]\n",
    "\n",
    "beam_configs = [\n",
    "    {'x': x, 'lbc': fixed, 'rbc': free}, \n",
    "    {'x': x, 'lbc': pinned, 'rbc': fixed},\n",
    "    {'x': x, 'lbc': free, 'rbc': pinned},\n",
    "    {'x': x, 'lbc': fixed, 'rbc': pinned},\n",
    "    {'x': x, 'lbc': pinned, 'rbc': free},\n",
    "    {'x': x, 'lbc': free, 'rbc': fixed},\n",
    "]\n",
    "\n",
    "data = BeamData(p, beam_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_condition(mask, data, u, u_x, m, m_x):\n",
    "    loss = nn.MSELoss()\n",
    "\n",
    "    bc_data = data[mask, 1:]\n",
    "    bc_mask = bc_data.bool()\n",
    "    u_bc, u_x_bc, m_bc, m_x_bc = u[mask], u_x[mask], m[mask], m_x[mask]\n",
    "    \n",
    "    bc_fixed = loss(u_bc[bc_mask[:, 0]], torch.ones_like(u_bc[bc_mask[:, 0]]))\n",
    "    bc_fixed += loss(u_x_bc[bc_mask[:, 0]], torch.ones_like(u_x_bc[bc_mask[:, 0]]))\n",
    "\n",
    "    bc_pinned = loss(u_bc[bc_mask[:, 1]], torch.ones_like(u_bc[bc_mask[:, 1]]))\n",
    "    bc_pinned += loss(m_bc[bc_mask[:, 1]], torch.ones_like(m_bc[bc_mask[:, 1]]))\n",
    "\n",
    "    bc_free = loss(m_bc[bc_mask[:, 2]], torch.ones_like(m_bc[bc_mask[:, 2]]))\n",
    "    bc_free += loss(m_x_bc[bc_mask[:, 2]], torch.ones_like(m_x_bc[bc_mask[:, 2]]))\n",
    "\n",
    "    total_loss = bc_fixed + bc_pinned + bc_free\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0., 1.]], grad_fn=<IndexBackward0>)\n",
      "tensor([[False,  True, False, False, False,  True],\n",
      "        [False, False,  True, False,  True, False],\n",
      "        [ True, False, False, False, False,  True]])\n",
      "tensor([[ 0.2451],\n",
      "        [-0.3408],\n",
      "        [-0.2196]], grad_fn=<IndexBackward0>)\n",
      "tensor([[-0.2196]], grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(data, batch_size=10, shuffle=True)\n",
    "loss = nn.MSELoss()\n",
    "for sample in dataloader:\n",
    "    x = sample[:, 0:1]\n",
    "    lbc = sample[:, 1:4]\n",
    "    rbc = sample[:, 4:]\n",
    "\n",
    "    out = model(sample)\n",
    "    u = out[:, 0:1]\n",
    "\n",
    "    mask_left = (sample[:, 0] == 0)\n",
    "    mask_right = (sample[:, 0] == 1)\n",
    "    \n",
    "    bc_data = sample[mask_left, 1:]\n",
    "    bc_mask = bc_data.bool()\n",
    "    print(bc_data)\n",
    "    print(bc_mask)\n",
    "    u_bc = u[mask_left]\n",
    "    print(u_bc)\n",
    "    a = u_bc[bc_mask[:, 0]]\n",
    "    print(a)\n",
    "\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__()\n",
    "        modules = []\n",
    "        for i in range(len(layers) - 2):  # Exclude last layer for activation\n",
    "            modules.append(nn.Linear(layers[i], layers[i+1]))\n",
    "            modules.append(nn.Tanh())\n",
    "        modules.append(nn.Linear(layers[-2], layers[-1]))  # Last layer (no activation)\n",
    "        self.network = nn.Sequential(*modules)\n",
    "\n",
    "        for layer in self.network:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                if layer.bias is not None:\n",
    "                    nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN():\n",
    "    def __init__(self, dataloader, layers):\n",
    "\n",
    "        self.dataloader = dataloader\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer_adam = torch.optim.Adam(self.dnn.parameters(), lr=0.01)\n",
    "\n",
    "    def model_value(self, data):\n",
    "        out = self.dnn(data)\n",
    "        u = out[:, 0:1]\n",
    "        m = out[:, 1:2]\n",
    "        return u, m\n",
    "\n",
    "    def boundary_condition(self, mask, data, u, u_x, m, m_x):\n",
    "        loss_fn = nn.MSELoss()\n",
    "\n",
    "        bc_data = data[mask, 1:]\n",
    "        bc_mask = bc_data.bool()\n",
    "        u_bc, u_x_bc, m_bc, m_x_bc = u[mask], u_x[mask], m[mask], m_x[mask]\n",
    "        \n",
    "        # Initialize losses\n",
    "        bc_fixed_loss = torch.tensor(0.0, device=device)\n",
    "        bc_pinned_loss = torch.tensor(0.0, device=device)\n",
    "        bc_free_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "        # Fixed BC\n",
    "        if u_bc[bc_mask[:, 0]].numel() > 0:\n",
    "            bc_fixed_loss = loss_fn(u_bc[bc_mask[:, 0]], torch.ones_like(u_bc[bc_mask[:, 0]]))\n",
    "            bc_fixed_loss += loss_fn(u_x_bc[bc_mask[:, 0]], torch.ones_like(u_x_bc[bc_mask[:, 0]]))\n",
    "\n",
    "        # Pinned BC\n",
    "        if u_bc[bc_mask[:, 1]].numel() > 0:\n",
    "            bc_pinned_loss = loss_fn(u_bc[bc_mask[:, 1]], torch.ones_like(u_bc[bc_mask[:, 1]]))\n",
    "            bc_pinned_loss += loss_fn(m_bc[bc_mask[:, 1]], torch.ones_like(m_bc[bc_mask[:, 1]]))\n",
    "\n",
    "        # Free BC\n",
    "        if m_bc[bc_mask[:, 2]].numel() > 0:\n",
    "            bc_free_loss = loss_fn(m_bc[bc_mask[:, 2]], torch.ones_like(m_bc[bc_mask[:, 2]]))\n",
    "            bc_free_loss += loss_fn(m_x_bc[bc_mask[:, 2]], torch.ones_like(m_x_bc[bc_mask[:, 2]]))\n",
    "\n",
    "        total_loss = bc_fixed_loss + bc_pinned_loss + bc_free_loss\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "    def loss_func(self, data):\n",
    "        u, m = self.model_value(data)\n",
    "\n",
    "        mask_left = (data[:, 0] == 0)\n",
    "        mask_right = (data[:, 1] == 1)\n",
    "\n",
    "        ones = torch.ones_like(u)\n",
    "        zeros = torch.zeros_like(u)\n",
    "\n",
    "        u_x = torch.autograd.grad(u, data, ones, create_graph=True)[0][:, 0:1]\n",
    "        u_2x = torch.autograd.grad(u_x, data, ones, create_graph=True)[0][:, 0:1]\n",
    "\n",
    "        m_x = torch.autograd.grad(m, data, ones, create_graph=True)[0][:, 0:1]\n",
    "        m_2x = torch.autograd.grad(m_x, data, ones, create_graph=True)[0][:, 0:1]\n",
    "\n",
    "        if True in mask_left:\n",
    "            bc_left_loss = self.boundary_condition(mask_left, data, u, u_x, m, m_x)\n",
    "        else:\n",
    "            bc_left_loss = torch.zeros(1, device=device)\n",
    "\n",
    "        if True in mask_right:\n",
    "            bc_right_loss = self.boundary_condition(mask_right, data, u, u_x, m, m_x)\n",
    "        else:\n",
    "            bc_right_loss = torch.zeros(1, device=device)\n",
    "\n",
    "        pde_loss = self.criterion(m_2x - 1, zeros)\n",
    "        pde_loss += self.criterion(u_2x + m, zeros)\n",
    "\n",
    "        return pde_loss, bc_left_loss, bc_right_loss\n",
    "        \n",
    "\n",
    "\n",
    "    def train(self, epochs=1000):\n",
    "        self.dnn.train()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for data_batch in self.dataloader:\n",
    "                pde_loss, bc_left_loss, bc_right_loss = self.loss_func(data_batch)\n",
    "                loss = pde_loss + bc_left_loss + bc_right_loss\n",
    "                \n",
    "                self.optimizer_adam.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer_adam.step()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}: Loss = {loss.item():.6f}\")\n",
    "                print(f\"PDE: {'{:e}'.format(pde_loss.item())}, BC left: {'{:e}'.format(bc_left_loss.item())}, BC right: {'{:e}'.format(bc_right_loss.item())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600, 7])\n",
      "[7, 64, 64, 64, 64, 64, 64, 64, 64, 2]\n"
     ]
    }
   ],
   "source": [
    "p = 100\n",
    "x = torch.linspace(0, 1, p).view(-1, 1)\n",
    "\n",
    "p_d = 100\n",
    "p_bc = 20\n",
    "x_l = np.zeros(p_bc)\n",
    "x_r = np.ones(p_bc)\n",
    "x = np.random.uniform(0, 1, p_d)\n",
    "x = np.append\n",
    "\n",
    "\n",
    "fixed = [1, 0, 0]\n",
    "pinned = [0, 1, 0]\n",
    "free = [0, 0, 1]\n",
    "\n",
    "nodes = 64\n",
    "layers = [7] + 8*[nodes] + [2]\n",
    "\n",
    "beam_configs = [\n",
    "    {'x': x, 'lbc': fixed, 'rbc': free}, \n",
    "    {'x': x, 'lbc': pinned, 'rbc': fixed},\n",
    "    {'x': x, 'lbc': free, 'rbc': pinned},\n",
    "    {'x': x, 'lbc': fixed, 'rbc': pinned},\n",
    "    {'x': x, 'lbc': pinned, 'rbc': free},\n",
    "    {'x': x, 'lbc': free, 'rbc': fixed},\n",
    "]\n",
    "\n",
    "data = BeamData(p, beam_configs)\n",
    "dataloader = DataLoader(data, batch_size=32, shuffle=True)\n",
    "print(dataloader.dataset.data.shape)\n",
    "print(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PINN(dataloader, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 2.485301\n",
      "PDE: 1.841802e+00, BC left: 0.000000e+00, BC right: 6.434993e-01\n",
      "Epoch 10: Loss = 2.339299\n",
      "PDE: 1.344702e+00, BC left: 0.000000e+00, BC right: 9.945975e-01\n",
      "Epoch 20: Loss = 0.982613\n",
      "PDE: 5.933039e-01, BC left: 0.000000e+00, BC right: 3.893091e-01\n",
      "Epoch 30: Loss = 2.032182\n",
      "PDE: 1.032193e+00, BC left: 0.000000e+00, BC right: 9.999892e-01\n",
      "Epoch 40: Loss = 2.164827\n",
      "PDE: 1.169160e+00, BC left: 0.000000e+00, BC right: 9.956663e-01\n",
      "Epoch 50: Loss = 2.169477\n",
      "PDE: 1.225068e+00, BC left: 0.000000e+00, BC right: 9.444093e-01\n",
      "Epoch 60: Loss = 2.011991\n",
      "PDE: 1.011898e+00, BC left: 0.000000e+00, BC right: 1.000093e+00\n",
      "Epoch 70: Loss = 2.031929\n",
      "PDE: 1.031927e+00, BC left: 0.000000e+00, BC right: 1.000002e+00\n",
      "Epoch 80: Loss = 2.013505\n",
      "PDE: 1.013372e+00, BC left: 0.000000e+00, BC right: 1.000132e+00\n",
      "Epoch 90: Loss = 2.179681\n",
      "PDE: 1.179708e+00, BC left: 0.000000e+00, BC right: 9.999723e-01\n",
      "Epoch 100: Loss = 2.410381\n",
      "PDE: 1.412155e+00, BC left: 0.000000e+00, BC right: 9.982263e-01\n",
      "Epoch 110: Loss = 2.055349\n",
      "PDE: 1.057896e+00, BC left: 0.000000e+00, BC right: 9.974525e-01\n",
      "Epoch 120: Loss = 2.027810\n",
      "PDE: 1.027789e+00, BC left: 0.000000e+00, BC right: 1.000021e+00\n",
      "Epoch 130: Loss = 3.016303\n",
      "PDE: 1.015269e+00, BC left: 1.000490e+00, BC right: 1.000543e+00\n",
      "Epoch 140: Loss = 2.006679\n",
      "PDE: 1.008381e+00, BC left: 0.000000e+00, BC right: 9.982982e-01\n",
      "Epoch 150: Loss = 2.633327\n",
      "PDE: 1.102602e+00, BC left: 4.957231e-01, BC right: 1.035002e+00\n",
      "Epoch 160: Loss = 2.588724\n",
      "PDE: 1.173314e+00, BC left: 4.069846e-01, BC right: 1.008425e+00\n",
      "Epoch 170: Loss = 1.988829\n",
      "PDE: 9.921434e-01, BC left: 0.000000e+00, BC right: 9.966857e-01\n",
      "Epoch 180: Loss = 1.992462\n",
      "PDE: 1.001298e+00, BC left: 0.000000e+00, BC right: 9.911642e-01\n",
      "Epoch 190: Loss = 2.037992\n",
      "PDE: 1.042904e+00, BC left: 0.000000e+00, BC right: 9.950875e-01\n",
      "Epoch 200: Loss = 1.947962\n",
      "PDE: 9.552795e-01, BC left: 0.000000e+00, BC right: 9.926829e-01\n",
      "Epoch 210: Loss = 1.919150\n",
      "PDE: 9.277660e-01, BC left: 0.000000e+00, BC right: 9.913841e-01\n",
      "Epoch 220: Loss = 1.911366\n",
      "PDE: 8.904709e-01, BC left: 0.000000e+00, BC right: 1.020895e+00\n",
      "Epoch 230: Loss = 1.851839\n",
      "PDE: 8.482931e-01, BC left: 0.000000e+00, BC right: 1.003546e+00\n",
      "Epoch 240: Loss = 1.846756\n",
      "PDE: 8.229640e-01, BC left: 0.000000e+00, BC right: 1.023793e+00\n",
      "Epoch 250: Loss = 1.783284\n",
      "PDE: 7.706053e-01, BC left: 0.000000e+00, BC right: 1.012679e+00\n",
      "Epoch 260: Loss = 1.850371\n",
      "PDE: 8.570949e-01, BC left: 0.000000e+00, BC right: 9.932762e-01\n",
      "Epoch 270: Loss = 1.623494\n",
      "PDE: 5.762608e-01, BC left: 0.000000e+00, BC right: 1.047233e+00\n",
      "Epoch 280: Loss = 1.648568\n",
      "PDE: 6.567215e-01, BC left: 0.000000e+00, BC right: 9.918469e-01\n",
      "Epoch 290: Loss = 2.921659\n",
      "PDE: 5.123500e-01, BC left: 1.343961e+00, BC right: 1.065348e+00\n",
      "Epoch 300: Loss = 1.299713\n",
      "PDE: 4.379706e-01, BC left: 0.000000e+00, BC right: 8.617426e-01\n",
      "Epoch 310: Loss = 1.326943\n",
      "PDE: 4.343436e-01, BC left: 0.000000e+00, BC right: 8.925995e-01\n",
      "Epoch 320: Loss = 1.116071\n",
      "PDE: 4.889293e-01, BC left: 0.000000e+00, BC right: 6.271418e-01\n",
      "Epoch 330: Loss = 0.976743\n",
      "PDE: 3.241428e-01, BC left: 0.000000e+00, BC right: 6.525997e-01\n",
      "Epoch 340: Loss = 0.999546\n",
      "PDE: 2.174344e-01, BC left: 1.167177e-01, BC right: 6.653934e-01\n",
      "Epoch 350: Loss = 1.239504\n",
      "PDE: 3.442453e-01, BC left: 2.335374e-01, BC right: 6.617215e-01\n",
      "Epoch 360: Loss = 0.743962\n",
      "PDE: 2.121167e-01, BC left: 0.000000e+00, BC right: 5.318452e-01\n",
      "Epoch 370: Loss = 0.746165\n",
      "PDE: 2.660146e-01, BC left: 0.000000e+00, BC right: 4.801507e-01\n",
      "Epoch 380: Loss = 0.972681\n",
      "PDE: 1.951348e-01, BC left: 0.000000e+00, BC right: 7.775462e-01\n",
      "Epoch 390: Loss = 0.940922\n",
      "PDE: 2.579028e-01, BC left: 0.000000e+00, BC right: 6.830188e-01\n",
      "Epoch 400: Loss = 0.922493\n",
      "PDE: 2.347543e-01, BC left: 0.000000e+00, BC right: 6.877385e-01\n",
      "Epoch 410: Loss = 0.757840\n",
      "PDE: 2.164266e-01, BC left: 0.000000e+00, BC right: 5.414131e-01\n",
      "Epoch 420: Loss = 0.889788\n",
      "PDE: 2.943147e-01, BC left: 0.000000e+00, BC right: 5.954730e-01\n",
      "Epoch 430: Loss = 1.131784\n",
      "PDE: 1.509775e-01, BC left: 6.273061e-01, BC right: 3.535002e-01\n",
      "Epoch 440: Loss = 1.033244\n",
      "PDE: 1.956982e-01, BC left: 0.000000e+00, BC right: 8.375456e-01\n",
      "Epoch 450: Loss = 0.595474\n",
      "PDE: 1.338512e-01, BC left: 0.000000e+00, BC right: 4.616226e-01\n",
      "Epoch 460: Loss = 0.689314\n",
      "PDE: 2.407935e-01, BC left: 0.000000e+00, BC right: 4.485203e-01\n",
      "Epoch 470: Loss = 1.100680\n",
      "PDE: 3.392936e-01, BC left: 0.000000e+00, BC right: 7.613866e-01\n",
      "Epoch 480: Loss = 0.780647\n",
      "PDE: 2.362078e-01, BC left: 0.000000e+00, BC right: 5.444390e-01\n",
      "Epoch 490: Loss = 3.177876\n",
      "PDE: 1.849781e+00, BC left: 0.000000e+00, BC right: 1.328094e+00\n",
      "Epoch 500: Loss = 2.055071\n",
      "PDE: 1.044693e+00, BC left: 0.000000e+00, BC right: 1.010378e+00\n",
      "Epoch 510: Loss = 2.860162\n",
      "PDE: 1.035882e+00, BC left: 8.126098e-01, BC right: 1.011671e+00\n",
      "Epoch 520: Loss = 2.056859\n",
      "PDE: 1.044087e+00, BC left: 0.000000e+00, BC right: 1.012773e+00\n",
      "Epoch 530: Loss = 2.046198\n",
      "PDE: 1.032875e+00, BC left: 0.000000e+00, BC right: 1.013323e+00\n",
      "Epoch 540: Loss = 2.041305\n",
      "PDE: 1.029610e+00, BC left: 0.000000e+00, BC right: 1.011695e+00\n",
      "Epoch 550: Loss = 2.830090\n",
      "PDE: 1.041813e+00, BC left: 7.771313e-01, BC right: 1.011145e+00\n",
      "Epoch 560: Loss = 2.044927\n",
      "PDE: 1.035863e+00, BC left: 0.000000e+00, BC right: 1.009064e+00\n",
      "Epoch 570: Loss = 2.026382\n",
      "PDE: 1.017179e+00, BC left: 0.000000e+00, BC right: 1.009203e+00\n",
      "Epoch 580: Loss = 2.051945\n",
      "PDE: 1.041619e+00, BC left: 0.000000e+00, BC right: 1.010327e+00\n",
      "Epoch 590: Loss = 2.035283\n",
      "PDE: 1.026230e+00, BC left: 0.000000e+00, BC right: 1.009053e+00\n",
      "Epoch 600: Loss = 2.061475\n",
      "PDE: 1.053594e+00, BC left: 0.000000e+00, BC right: 1.007881e+00\n",
      "Epoch 610: Loss = 3.616102\n",
      "PDE: 1.033997e+00, BC left: 1.581635e+00, BC right: 1.000469e+00\n",
      "Epoch 620: Loss = 2.030041\n",
      "PDE: 1.029345e+00, BC left: 0.000000e+00, BC right: 1.000695e+00\n",
      "Epoch 630: Loss = 2.026028\n",
      "PDE: 1.025703e+00, BC left: 0.000000e+00, BC right: 1.000325e+00\n",
      "Epoch 640: Loss = 2.045161\n",
      "PDE: 1.039625e+00, BC left: 0.000000e+00, BC right: 1.005537e+00\n",
      "Epoch 650: Loss = 2.028049\n",
      "PDE: 1.026173e+00, BC left: 0.000000e+00, BC right: 1.001876e+00\n",
      "Epoch 660: Loss = 2.037886\n",
      "PDE: 1.037835e+00, BC left: 0.000000e+00, BC right: 1.000050e+00\n",
      "Epoch 670: Loss = 2.105757\n",
      "PDE: 1.098653e+00, BC left: 0.000000e+00, BC right: 1.007104e+00\n",
      "Epoch 680: Loss = 2.754492\n",
      "PDE: 1.021878e+00, BC left: 7.305705e-01, BC right: 1.002043e+00\n",
      "Epoch 690: Loss = 2.021470\n",
      "PDE: 1.019555e+00, BC left: 0.000000e+00, BC right: 1.001915e+00\n",
      "Epoch 700: Loss = 4.716649\n",
      "PDE: 1.034054e+00, BC left: 2.682369e+00, BC right: 1.000225e+00\n",
      "Epoch 710: Loss = 2.077259\n",
      "PDE: 1.076952e+00, BC left: 0.000000e+00, BC right: 1.000307e+00\n",
      "Epoch 720: Loss = 2.010296\n",
      "PDE: 1.006492e+00, BC left: 0.000000e+00, BC right: 1.003804e+00\n",
      "Epoch 730: Loss = 2.015863\n",
      "PDE: 1.014781e+00, BC left: 0.000000e+00, BC right: 1.001082e+00\n",
      "Epoch 740: Loss = 2.009119\n",
      "PDE: 1.007860e+00, BC left: 0.000000e+00, BC right: 1.001259e+00\n",
      "Epoch 750: Loss = 2.108186\n",
      "PDE: 1.106024e+00, BC left: 0.000000e+00, BC right: 1.002162e+00\n",
      "Epoch 760: Loss = 2.682942\n",
      "PDE: 1.038318e+00, BC left: 6.445829e-01, BC right: 1.000041e+00\n",
      "Epoch 770: Loss = 2.007449\n",
      "PDE: 1.007286e+00, BC left: 0.000000e+00, BC right: 1.000163e+00\n",
      "Epoch 780: Loss = 2.008080\n",
      "PDE: 1.007796e+00, BC left: 0.000000e+00, BC right: 1.000284e+00\n",
      "Epoch 790: Loss = 2.009532\n",
      "PDE: 1.008853e+00, BC left: 0.000000e+00, BC right: 1.000679e+00\n",
      "Epoch 800: Loss = 2.078333\n",
      "PDE: 1.078232e+00, BC left: 0.000000e+00, BC right: 1.000101e+00\n",
      "Epoch 810: Loss = 2.037626\n",
      "PDE: 1.037494e+00, BC left: 0.000000e+00, BC right: 1.000131e+00\n",
      "Epoch 820: Loss = 2.001332\n",
      "PDE: 1.001323e+00, BC left: 0.000000e+00, BC right: 1.000009e+00\n",
      "Epoch 830: Loss = 2.015020\n",
      "PDE: 1.014903e+00, BC left: 0.000000e+00, BC right: 1.000117e+00\n",
      "Epoch 840: Loss = 2.002124\n",
      "PDE: 1.002104e+00, BC left: 0.000000e+00, BC right: 1.000020e+00\n",
      "Epoch 850: Loss = 3.070729\n",
      "PDE: 1.070683e+00, BC left: 1.000025e+00, BC right: 1.000022e+00\n",
      "Epoch 860: Loss = 2.021937\n",
      "PDE: 1.021888e+00, BC left: 0.000000e+00, BC right: 1.000049e+00\n",
      "Epoch 870: Loss = 2.021539\n",
      "PDE: 1.021535e+00, BC left: 0.000000e+00, BC right: 1.000004e+00\n",
      "Epoch 880: Loss = 2.081274\n",
      "PDE: 1.081273e+00, BC left: 0.000000e+00, BC right: 1.000001e+00\n",
      "Epoch 890: Loss = 2.013973\n",
      "PDE: 1.013964e+00, BC left: 0.000000e+00, BC right: 1.000009e+00\n",
      "Epoch 900: Loss = 2.036177\n",
      "PDE: 1.036177e+00, BC left: 0.000000e+00, BC right: 1.000000e+00\n",
      "Epoch 910: Loss = 3.011670\n",
      "PDE: 1.011661e+00, BC left: 1.000005e+00, BC right: 1.000005e+00\n",
      "Epoch 920: Loss = 2.147816\n",
      "PDE: 1.147813e+00, BC left: 0.000000e+00, BC right: 1.000002e+00\n",
      "Epoch 930: Loss = 3.020291\n",
      "PDE: 1.020290e+00, BC left: 1.000000e+00, BC right: 1.000001e+00\n",
      "Epoch 940: Loss = 2.097006\n",
      "PDE: 1.097006e+00, BC left: 0.000000e+00, BC right: 1.000000e+00\n",
      "Epoch 950: Loss = 3.567373\n",
      "PDE: 1.085296e+00, BC left: 1.482072e+00, BC right: 1.000004e+00\n",
      "Epoch 960: Loss = 2.054546\n",
      "PDE: 1.054534e+00, BC left: 0.000000e+00, BC right: 1.000012e+00\n",
      "Epoch 970: Loss = 2.016258\n",
      "PDE: 1.016257e+00, BC left: 0.000000e+00, BC right: 1.000001e+00\n",
      "Epoch 980: Loss = 2.000310\n",
      "PDE: 1.000310e+00, BC left: 0.000000e+00, BC right: 1.000000e+00\n",
      "Epoch 990: Loss = 3.626896\n",
      "PDE: 1.113500e+00, BC left: 1.513391e+00, BC right: 1.000005e+00\n",
      "CPU times: total: 25min 9s\n",
      "Wall time: 3min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 1]\n",
      "tensor([1, 0, 1, 1])\n",
      "tensor([False,  True, False, False])\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 0, 1, 1])\n",
    "print(a)\n",
    "\n",
    "t = torch.tensor(a)\n",
    "print(t)\n",
    "\n",
    "mask = (t == 0)\n",
    "print(mask)\n",
    "\n",
    "if True in mask:\n",
    "    print(\"yes\")\n",
    "else:\n",
    "    print(\"no\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
