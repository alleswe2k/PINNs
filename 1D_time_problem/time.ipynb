{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.animation as animation\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA support\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     print(\"CUDA\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "#     print(\"CPU\")\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.layers.append(nn.Linear(layers[i], layers[i+1]))\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.activation(layer(x))\n",
    "        return self.layers[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbfgs_lr = 0.01\n",
    "adam_lr = 0.1\n",
    "\n",
    "w_pde = 0.1\n",
    "w_bc = 1.0\n",
    "w_ic = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN():\n",
    "    def __init__(self, X, layers, lb, rb, f, ic1, ic2):\n",
    "        \n",
    "        self.x_all = X[\"all\"][0]\n",
    "        self.t_all = X[\"all\"][1]\n",
    "\n",
    "        self.x_bc_lb = X[\"bc_lb\"][0]\n",
    "        self.t_bc_lb = X[\"bc_lb\"][1]\n",
    "        self.x_bc_rb = X[\"bc_rb\"][0]\n",
    "        self.t_bc_rb = X[\"bc_rb\"][1]\n",
    "\n",
    "        self.x_ic = X[\"ic\"][0]\n",
    "        self.t_ic = X[\"ic\"][0]\n",
    "\n",
    "        self.f = f\n",
    "        self.ic1 = ic1\n",
    "        self.ic2 = ic2\n",
    "\n",
    "        self.lb = lb\n",
    "        self.rb = rb\n",
    "\n",
    "        # DNN\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer_lbfgs = torch.optim.LBFGS(\n",
    "            self.dnn.parameters(),\n",
    "            lr=lbfgs_lr,\n",
    "            max_iter=50000,\n",
    "            max_eval=50000,\n",
    "            history_size=50,\n",
    "            tolerance_grad=1e-5,\n",
    "            tolerance_change=1.0 * np.finfo(float).eps,\n",
    "            line_search_fn=\"strong_wolfe\"\n",
    "        )\n",
    "\n",
    "        self.optimizer_adam = torch.optim.Adam(self.dnn.parameters(), lr=adam_lr)\n",
    "        # self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer_adam, step_size=1000, gamma=0.5)\n",
    "        self.iter = 0\n",
    "\n",
    "    def model_value(self, x, t):\n",
    "        u = self.dnn(torch.stack([x, t], dim=1))\n",
    "        return u\n",
    "    \n",
    "    def space_derivative(self, u, x):\n",
    "        u_x = torch.autograd.grad(u, x, torch.ones_like(u), create_graph=True)[0]\n",
    "        u_2x = torch.autograd.grad(u_x, x, torch.ones_like(u_x), create_graph=True)[0]\n",
    "        u_3x = torch.autograd.grad(u_2x, x, torch.ones_like(u_2x), create_graph=True)[0]\n",
    "        u_4x = torch.autograd.grad(u_3x, x, torch.ones_like(u_3x), create_graph=True)[0]      \n",
    "\n",
    "        return u_x, u_2x, u_3x, u_4x\n",
    "    \n",
    "    def time_derivative(self, u , t):\n",
    "        u_t = torch.autograd.grad(u, t, torch.ones_like(u), create_graph=True)[0]\n",
    "        u_2t = torch.autograd.grad(u_t, t, torch.ones_like(u_t), create_graph=True)[0]\n",
    "\n",
    "        return u_t, u_2t\n",
    "\n",
    "    def boundary_condition(self, cond, x, t):\n",
    "        u = self.model_value(x, t)\n",
    "        bc_loss = 0\n",
    "        u_x, u_2x, u_3x, _ = self.space_derivative(u, x)\n",
    "\n",
    "        match cond:\n",
    "            case 'pinned':\n",
    "                bc_loss += torch.mean(u**2) + torch.mean(u_2x**2)\n",
    "            case 'fixed':\n",
    "                bc_loss += torch.mean(u**2) + torch.mean(u_x**2)\n",
    "            case 'free':\n",
    "                bc_loss += torch.mean(u_2x**2) + torch.mean(u_3x**2)\n",
    "            case 'roller':\n",
    "                bc_loss += torch.mean(u_x**2) + torch.mean(u_3x**2)\n",
    "\n",
    "        return bc_loss\n",
    "    \n",
    "    def initial_condition(self, x, t):\n",
    "        u = self.model_value(x, t)\n",
    "        u_t, _ = self.time_derivative(u, t)\n",
    "\n",
    "        ic_loss = torch.mean((u - self.ic1)**2)\n",
    "\n",
    "        ic_loss += torch.mean((u_t - self.ic2)**2)\n",
    " \n",
    "        return ic_loss\n",
    "\n",
    "    def pde(self, x, t):\n",
    "        u = self.model_value(x, t)\n",
    "        _, _, _, u_4x = self.space_derivative(u, x)\n",
    "        _, u_2t = self.time_derivative(u, t)\n",
    "        \n",
    "        pde_loss = torch.mean((u_2t + u_4x - self.f)**2)\n",
    "\n",
    "        return pde_loss\n",
    "\n",
    "    def loss_func(self):\n",
    "        pde_loss = self.pde(self.x_all, self.t_all)\n",
    "        bc_loss = self.boundary_condition(self.lb, self.x_bc_lb, self.t_bc_lb)\n",
    "        bc_loss += self.boundary_condition(self.rb, self.x_bc_rb, self.t_bc_rb)\n",
    "        ic_loss = self.initial_condition(self.x_ic, self.t_ic)\n",
    "\n",
    "        # total_loss = w_pde*pde_loss + w_bc*bc_loss + w_ic*ic_loss\n",
    "\n",
    "        return pde_loss, bc_loss, ic_loss\n",
    "\n",
    "\n",
    "    def lbfgs_func(self):\n",
    "        # loss = self.loss_func()\n",
    "        pde_loss, bc_loss, ic_loss = self.loss_func()\n",
    "        loss = w_pde*pde_loss + w_bc*bc_loss + w_ic*ic_loss\n",
    "\n",
    "        self.optimizer_lbfgs.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        self.iter += 1\n",
    "        if self.iter % 100 == 0:\n",
    "            print(f\"Iter: {self.iter}, Loss: {'{:e}'.format(loss.item())}\")\n",
    "            print(f\"PDE: {'{:e}'.format(pde_loss.item())}, BC: {'{:e}'.format(bc_loss.item())}, IC: {'{:e}'.format(ic_loss.item())}\")\n",
    "        return loss\n",
    "    \n",
    "    def train(self, epochs=1000):\n",
    "        self.dnn.train()\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            # loss = self.loss_func()\n",
    "            pde_loss, bc_loss, ic_loss = self.loss_func()\n",
    "            loss = w_pde*pde_loss + w_bc*bc_loss + w_ic*ic_loss\n",
    "\n",
    "\n",
    "            self.optimizer_adam.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            self.optimizer_adam.step()\n",
    "            # self.scheduler.step()\n",
    "\n",
    "            if epoch % 200 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {'{:e}'.format(loss.item())}\")\n",
    "                print(f\"PDE: {'{:e}'.format(pde_loss.item())}, BC: {'{:e}'.format(bc_loss.item())}, IC: {'{:e}'.format(ic_loss.item())}\")\n",
    "                # print(f\"Current LR: {self.scheduler.get_last_lr()[0]}\")\n",
    "        self.optimizer_lbfgs.step(self.lbfgs_func)\n",
    "\n",
    "    def predict(self, x, t):\n",
    "        self.dnn.eval()\n",
    "        u = self.model_value(x, t)\n",
    "        u = u.detach().cpu().numpy()\n",
    "\n",
    "        # final_loss = self.loss_func()\n",
    "        # final_loss = final_loss.detach().cpu().numpy()\n",
    "        return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x, t):\n",
    "    return (1-16*np.pi**2) * np.sin(x) * np.cos(4*np.pi*t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_points(points):\n",
    "    def ic_points(n_points):\n",
    "        x = torch.rand(n_points, device=device).requires_grad_()\n",
    "        t = torch.zeros(n_points, device=device).requires_grad_()\n",
    "        return x, t\n",
    "\n",
    "    def bc_points(n_points, x_bc):\n",
    "        x = torch.ones(n_points, device=device).requires_grad_() * x_bc\n",
    "        t = torch.rand(n_points, device=device).requires_grad_()\n",
    "        return x, t\n",
    "\n",
    "    def domain_points(n_points):\n",
    "        x = torch.rand(n_points, device=device).requires_grad_()\n",
    "        t = torch.rand(n_points, device=device).requires_grad_()\n",
    "        return x, t\n",
    "\n",
    "    x_d, t_d = domain_points(points[0])\n",
    "    x_bc_lb, t_bc_lb = bc_points(points[1], 0)\n",
    "    x_bc_rb, t_bc_rb = bc_points(points[1], torch.pi)\n",
    "    x_ic, t_ic = ic_points(points[2])\n",
    "\n",
    "    x_all = torch.cat([x_d, x_bc_lb, x_bc_rb, x_ic], dim=0)\n",
    "    t_all = torch.cat([t_d, t_bc_lb, t_bc_rb, t_ic], dim=0)\n",
    "\n",
    "\n",
    "    generated_points = {\"all\": [x_all, t_all], \"bc_lb\": [x_bc_lb, t_bc_lb], \"bc_rb\": [x_bc_rb, t_bc_rb], \"ic\": [x_ic, t_ic]}\n",
    "    return generated_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [1000, 200, 200]\n",
    "collocation = generate_points(points)\n",
    "\n",
    "\n",
    "all_x = collocation[\"all\"][0]\n",
    "all_t = collocation[\"all\"][1]\n",
    "\n",
    "ic_x = collocation[\"ic\"][0]\n",
    "\n",
    "f = lambda x, t: (1 - 16*torch.pi**2)*torch.sin(x)*torch.cos(4*torch.pi*t)\n",
    "ic1 = lambda x: torch.sin(x)\n",
    "ic2 = lambda x: torch.full_like(x, 0)\n",
    "\n",
    "vec1 = ic1(ic_x)\n",
    "vec2 = ic2(ic_x)\n",
    "fun = f(all_x, all_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = 32\n",
    "layers = [2, nodes, nodes, nodes, nodes, 1]\n",
    "lb = 'pinned'\n",
    "rb = 'pinned'\n",
    "\n",
    "model = PINN(collocation, layers, lb, rb, fun, vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 3297\n",
      "Collocation points: 1600\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.dnn.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {num_params}\")\n",
    "print(f\"Collocation points: {len(collocation[\"all\"][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4000 [00:00<?, ?it/s]c:\\Users\\alexa\\miniconda3\\envs\\pinn\\Lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\cuda\\CublasHandlePool.cpp:135.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "  0%|          | 2/4000 [00:01<30:32,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.973328e+02\n",
      "PDE: 2.971058e+03, BC: 9.635971e-02, IC: 1.305914e-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 202/4000 [00:31<08:42,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200, Loss: 2.973419e+02\n",
      "PDE: 2.971302e+03, BC: 5.085716e-02, IC: 1.608829e-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 349/4000 [00:55<09:37,  6.32it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 129\u001b[0m, in \u001b[0;36mPINN.train\u001b[1;34m(self, epochs)\u001b[0m\n\u001b[0;32m    125\u001b[0m loss \u001b[38;5;241m=\u001b[39m w_pde\u001b[38;5;241m*\u001b[39mpde_loss \u001b[38;5;241m+\u001b[39m w_bc\u001b[38;5;241m*\u001b[39mbc_loss \u001b[38;5;241m+\u001b[39m w_ic\u001b[38;5;241m*\u001b[39mic_loss\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_adam\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 129\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_adam\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# self.scheduler.step()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\miniconda3\\envs\\pinn\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\miniconda3\\envs\\pinn\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\miniconda3\\envs\\pinn\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# d = domain_points(2000)\n",
    "# bc_lb = bc_points(200, 0)\n",
    "# bc_rb = bc_points(400, 1)\n",
    "# ic = ic_points(200)\n",
    "\n",
    "# x1 = d[:, 0].cpu().numpy()\n",
    "# t1 = d[:, 1].cpu().numpy()\n",
    "\n",
    "# x2 = bc_lb[:, 0].cpu().numpy()\n",
    "# t2 = bc_lb[:, 1].cpu().numpy()\n",
    "\n",
    "# x3 = bc_rb[:, 0].cpu().numpy()\n",
    "# t3 = bc_rb[:, 1].cpu().numpy()\n",
    "\n",
    "# x4 = ic[:, 0].cpu().numpy()\n",
    "# t4 = ic[:, 1].cpu().numpy()\n",
    "\n",
    "# plt.scatter(x1, t1, s=10, color=\"blue\", alpha=0.5)\n",
    "# plt.scatter(x2, t2, s=10, color=\"red\", alpha=0.5)\n",
    "# plt.scatter(x3, t3, s=10, color=\"red\", alpha=0.5)\n",
    "# plt.scatter(x4, t4, s=10, color=\"green\", alpha=0.5)\n",
    "# plt.xlabel(\"x\")\n",
    "# plt.ylabel(\"t\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exact solution\n",
    "\n",
    "def exact_solution(x, t):\n",
    "    return np.sin(x)*np.cos(4*np.pi*t)\n",
    "\n",
    "\n",
    "x = np.linspace(0, np.pi, 100)\n",
    "t = np.linspace(0, 1, 100)\n",
    "\n",
    "X, T = np.meshgrid(x, t)\n",
    "U = np.sin(X) * np.cos(4 * np.pi * T) \n",
    "\n",
    "plt.imshow(U)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# # Set up the figure and axis\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.set_xlim(0, np.pi)\n",
    "# ax.set_ylim(-1, 1)\n",
    "# ax.set_xlabel(\"x\")\n",
    "# ax.set_ylabel(\"u(x, t)\")\n",
    "# ax.set_title(\"Animation of Exact Solution\")\n",
    "\n",
    "\n",
    "# # Initialize the line object\n",
    "# line, = ax.plot([], [], lw=2)\n",
    "\n",
    "# # Initialization function: Clears the line data\n",
    "# def init():\n",
    "#     line.set_data([], [])\n",
    "#     return line,\n",
    "\n",
    "# # Update function for each frame\n",
    "# def update(frame):\n",
    "#     y = exact_solution(x, t[frame])  # Compute solution at current time step\n",
    "#     line.set_data(x, y)\n",
    "#     return line,\n",
    "\n",
    "# # Create the animation\n",
    "# ani = animation.FuncAnimation(fig, update, frames=len(t), init_func=init, blit=True, interval=50)\n",
    "\n",
    "# ani.save(\"wave_animation.gif\", writer=\"pillow\", fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "           \n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0) \n",
    "\n",
    "print(X_star)\n",
    "print(lb)\n",
    "print(ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the resolution of the grid\n",
    "x_min, x_max = 0, np.pi  # X range\n",
    "t_min, t_max = 0, 1  # Time range\n",
    "grid_size = 100  # Number of points in each dimension\n",
    "\n",
    "# Create a meshgrid\n",
    "x_vals = np.linspace(x_min, x_max, grid_size)\n",
    "t_vals = np.linspace(t_min, t_max, grid_size)\n",
    "X, T = np.meshgrid(x_vals, t_vals)  # Create a 2D meshgrid\n",
    "\n",
    "# Flatten the meshgrid and convert to PyTorch tensor\n",
    "X_flat = torch.tensor(X.flatten(), dtype=torch.float32, device=device)\n",
    "T_flat = torch.tensor(T.flatten(), dtype=torch.float32, device=device)\n",
    "\n",
    "u_exact = torch.sin(X_flat) * torch.cos(4 * torch.pi * T_flat)\n",
    "u_exact = u_exact.detach().cpu().numpy()\n",
    "u_pred = model.predict(X_flat, T_flat).flatten()\n",
    "abs_error = np.abs(u_pred - u_exact)\n",
    "l2_error = np.linalg.norm(u_pred - u_exact) / np.linalg.norm(u_exact, 2)\n",
    "\n",
    "\n",
    "# Reshape predictions back into a 2D grid\n",
    "U_pred_reshaped = u_pred.reshape(grid_size, grid_size)\n",
    "\n",
    "abs_error_reshape = abs_error.reshape(grid_size, grid_size)\n",
    "\n",
    "print(l2_error)\n",
    "print(abs_error)\n",
    "\n",
    "# Plot the result\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(abs_error_reshape)\n",
    "plt.colorbar(label=\"Predicted Output (U)\")\n",
    "plt.xlabel(\"x-coordinate\")\n",
    "plt.ylabel(\"Time (t)\")\n",
    "plt.title(\"PINN Predicted Solution\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
